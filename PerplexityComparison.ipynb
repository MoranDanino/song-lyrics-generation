{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Perplexity Comparison:\n"
      ],
      "metadata": {
        "id": "2xod8TqQwNwz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASSV-wz0fqAN"
      },
      "outputs": [],
      "source": [
        "# # if needed:\n",
        "# install dependencies\n",
        "# !pip install torch\n",
        "# !pip install transformers[torch]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import math\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # if using GPU\n",
        "\n",
        "# set transformer seed\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "2Y0CMXobfumB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# define path to folder - CHANGE TO WHICHEVER DIRECTORY YOU WANT\n",
        "path_folder = f\"/content/drive/MyDrive/Deep_Learning_project/splits\"\n",
        "path_model = f\"/content/drive/MyDrive/Deep_Learning_project/model\"\n",
        "\n",
        "path_folder_tokenizer = f\"/content/drive/MyDrive/Deep_Learning_project/splits/tokenizer\""
      ],
      "metadata": {
        "id": "aa9WeQmafuhP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6608caf7-01f1-4706-bbc1-97ee01b30841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids_path = path_folder+'/all_input_ids.npy'\n",
        "attn_masks_path = path_folder+'/all_attention_masks.npy'"
      ],
      "metadata": {
        "id": "Tw4GNc0yfueL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load untuned model\n",
        "model_name = \"gpt2\"\n",
        "model_untuned = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "\n",
        "# load tuned model\n",
        "tokenizer = AutoTokenizer.from_pretrained(path_folder_tokenizer)\n",
        "model = AutoModelForCausalLM.from_pretrained(path_model)\n",
        "\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model_untuned.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# move model to GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # if using GPU\n",
        "model_untuned = model_untuned.to(device)\n",
        "model = model.to(device)"
      ],
      "metadata": {
        "id": "S5uGxPJDfubd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4779c3-fe47-413d-b431-cc849eacc373"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# data set class, implimenting __init__, __len__, __getitem__ for dataloader purposes\n",
        "class songsDataset:\n",
        "    def __init__(self, input_ids, attn_masks):\n",
        "        self.input_ids = input_ids\n",
        "        self.attn_masks = attn_masks\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.input_ids[idx], device=device), torch.tensor(self.attn_masks[idx], device=device)"
      ],
      "metadata": {
        "id": "Mc5IRI79fuZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load input ids and attention masks\n",
        "input_ids = np.load(input_ids_path)\n",
        "attn_masks = np.load(attn_masks_path)\n",
        "\n",
        "# split into training and validation again by using the same random state that we used for training\n",
        "input_ids_train, input_ids_val = train_test_split(input_ids, test_size = 0.0005, random_state=123)\n",
        "attn_masks_train, attn_masks_val = train_test_split(attn_masks, test_size = 0.0005, random_state=123)\n",
        "\n",
        "# create instances of validation dataset\n",
        "dataset_val = songsDataset(input_ids_val, attn_masks_val)\n",
        "\n",
        "print(\"len of val dataset:\", len(dataset_val))"
      ],
      "metadata": {
        "id": "JaIe3_DvfuWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee111038-35c7-419a-fe13-85964f86faf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "len of val dataset: 139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define dataloader\n",
        "validation_dataloader = DataLoader(dataset_val, batch_size = 1)"
      ],
      "metadata": {
        "id": "nmiTWnKRfuR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use validation loop on the model to calculate perplexity\n",
        "def perplexity_model(validation_loader, m):\n",
        "    total_perplexity = 0\n",
        "    val_steps = len(validation_dataloader)\n",
        "\n",
        "    # iterate over validation set\n",
        "    for ids, masks in validation_dataloader:\n",
        "        with torch.no_grad():  # disable gradients for validation\n",
        "            outputs = m(ids, attention_mask = masks, labels = ids)  # run model forward\n",
        "\n",
        "            loss = outputs[0]\n",
        "            batch_loss = loss.item()\n",
        "            total_perplexity += math.exp(batch_loss)  # calculate perplexity = exponent of loss\n",
        "\n",
        "    average_perplexity = total_perplexity / val_steps\n",
        "\n",
        "    return average_perplexity  # return average perplexity of model - exponent of average loss"
      ],
      "metadata": {
        "id": "l9ZRWZHQfuPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare perplexity\n",
        "perplexity_untuned = perplexity_model(validation_dataloader, model_untuned)\n",
        "perplexity_tuned = perplexity_model(validation_dataloader, model)\n",
        "print(f\"Average perplexity of untuned model over validation dataet: {perplexity_untuned}\")\n",
        "print(f\"Average perplexity of tuned model over validation dataet: {perplexity_tuned}\")"
      ],
      "metadata": {
        "id": "2IGb6zf5fuKN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cb56cd3-f481-422f-afd4-1901219a7a3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average perplexity of untuned model over validation dataet: 5.676198831197441e+51\n",
            "Average perplexity of tuned model over validation dataet: 8.476248169339895\n"
          ]
        }
      ]
    }
  ]
}