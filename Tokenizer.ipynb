{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenize:\n"
      ],
      "metadata": {
        "id": "PyM7Aw94vj1k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eoED3HPYz-X"
      },
      "outputs": [],
      "source": [
        "# # if needed:\n",
        "# install dependencies\n",
        "# !pip install torch\n",
        "# !pip install transformers[torch]\n",
        "# !pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "import torch\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "from google.colab import drive\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from torch.utils.data import random_split, DataLoader\n",
        "import gc\n",
        "import math\n",
        "\n",
        "np.random.seed(123)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # if using GPU"
      ],
      "metadata": {
        "id": "DLsQyPj5Y26n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# define path to folder - CHANGE TO WHICHEVER DIRECTORY YOU WANT\n",
        "path_folder_in = f\"/content/drive/MyDrive/Deep_Learning_project/chunks\"\n",
        "path_folder_out = f\"/content/drive/MyDrive/Deep_Learning_project/splits\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VA0UOkUCY22K",
        "outputId": "68734b07-5b27-464a-e397-81c6a04d319d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# load pre-trained CausalLM GPT-2 model and the matching tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\", bos_token='<sos>', eos_token='<eos>', pad_token='<pad>')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VJmB-XasY2zp",
        "outputId": "e866ca81-9295-4ce0-ed21-5c8949da45c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# add special tokens again, ensure they are not already added\n",
        "if '[Genre: Pop]' not in tokenizer.get_vocab():\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens': ['[Genre: Pop]']})\n",
        "if '[Genre: Rock]' not in tokenizer.get_vocab():\n",
        "    tokenizer.add_special_tokens({'additional_special_tokens': ['[Genre: Rap]']})\n",
        "\n",
        "# check the tokens were added\n",
        "print('[Genre: Pop]' in tokenizer.get_vocab())\n",
        "print('[Genre: Rap]' in tokenizer.get_vocab())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PG2cTQZCjvGr",
        "outputId": "77e03604-d7d3-45e9-a7ef-e87bccb1144d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define your preprocessing function\n",
        "def preprocess_csv(filename, tokenizer):\n",
        "  df = pd.read_csv(filename)\n",
        "  df = df.dropna()\n",
        "\n",
        "  # filter out lyrics with more than 400 words and less then 20 words\n",
        "  df = df[df['lyric'].apply(lambda x: len(tokenizer.encode(x)) < 400)]\n",
        "  df = df[df['lyric'].apply(lambda x: len(tokenizer.encode(x)) > 20)]\n",
        "\n",
        "  # downsample pop chunk to size of rap chunk\n",
        "  rap_count = (df['class'] == 0).sum()\n",
        "  pop_count = (df['class'] == 1).sum()\n",
        "\n",
        "  if pop_count > rap_count:\n",
        "      pop_indices = df[df['class'] == 1].index\n",
        "      drop_indices = np.random.choice(pop_indices, size=(pop_count - rap_count), replace=False) # randomly select indices to *drop* to match the number of rap songs\n",
        "      df = df.drop(drop_indices)   # drop the selected pop songs- now the number of pop and rap songs are equal\n",
        "      # print(\"Pop chunk downsampled to match rap chunk:\\n\", df['class'].value_counts())\n",
        "\n",
        "  # convert lyrics to list\n",
        "  songs = df[\"lyric\"].tolist()\n",
        "\n",
        "  # initialize lists to store input ids and attention masks\n",
        "  input_ids = []\n",
        "  attention_masks = []\n",
        "\n",
        "  # iterate over every song and encode it using the tokenizer\n",
        "  for song in songs:\n",
        "      encodings = tokenizer('<sos>' + song + '<eos>', truncation=True, max_length= 400, padding=\"max_length\")\n",
        "      input_ids.append(torch.tensor(encodings['input_ids']))\n",
        "      attention_masks.append(torch.tensor(encodings['attention_mask']))\n",
        "\n",
        "  return input_ids, attention_masks"
      ],
      "metadata": {
        "id": "nsRSWFjPY2xR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_chunks = 40\n",
        "\n",
        "# list to store results from each file\n",
        "all_input_ids = []\n",
        "all_attention_masks = []\n",
        "\n",
        "# loop over each CSV file\n",
        "for i in range(1, num_chunks+1):  # we have 20 CSV files\n",
        "    # construct filename\n",
        "    filename = f\"{path_folder_in}/chunk_{i}.csv\"\n",
        "    print(i)\n",
        "    # preprocess CSV file\n",
        "    input_ids, attention_masks = preprocess_csv(filename, tokenizer)\n",
        "\n",
        "    # append results to the lists\n",
        "    all_input_ids.extend(input_ids)\n",
        "    all_attention_masks.extend(attention_masks)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPu8YUGgY2u3",
        "outputId": "1ea9e0cc-9ba2-4f83-879e-2bf4d708b106"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (1267 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# concatenate the lists into tensors and save the files\n",
        "np.save(f\"{path_folder_out}/all_input_ids.npy\", np.array(all_input_ids))\n",
        "np.save(f\"{path_folder_out}/all_attention_masks.npy\", np.array(all_attention_masks))"
      ],
      "metadata": {
        "id": "WMvmPTOpY2sV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(f'{path_folder_out}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nw1keFbNlmYy",
        "outputId": "40761388-7f52-4d0d-8346-3d2b55d31a25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/Deep_Learning_project/splits/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/Deep_Learning_project/splits/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/Deep_Learning_project/splits/vocab.json',\n",
              " '/content/drive/MyDrive/Deep_Learning_project/splits/merges.txt',\n",
              " '/content/drive/MyDrive/Deep_Learning_project/splits/added_tokens.json',\n",
              " '/content/drive/MyDrive/Deep_Learning_project/splits/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    }
  ]
}